{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install keras_cv datasets transformers tensorboard tensorflow ipywidgets opencv-python tensorflow-datasets scikit-learn\n",
    "!git-lfs --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to huggingface if first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the GPU is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.config.experimental import list_physical_devices\n",
    "print(list_physical_devices('GPU'))\n",
    "\n",
    "model_id = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the database, also this is the time to define data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "from tensorflow import device\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_cv.layers import RandAugment\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model ID and other parameters\n",
    "num_layers = 2\n",
    "magnitude = 0.15\n",
    "\n",
    "# Load the ViTImageProcessor\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create RandAugment transformation\n",
    "rand_augment = RandAugment(\n",
    "    value_range=[-1,1],\n",
    "    augmentations_per_image=num_layers,\n",
    "    magnitude=magnitude,\n",
    ")\n",
    "\n",
    "\n",
    "def transform(batch):\n",
    "    inputs = image_processor([x for x in batch[\"image\"]], return_tensors=\"tf\")\n",
    "    inputs[\"labels\"] = batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "def augment(batch):\n",
    "    inputs = image_processor([x for x in batch[\"image\"]], return_tensors=\"tf\")\n",
    "    transposed = tf.transpose(inputs[\"pixel_values\"], perm=[0,3,2,1])\n",
    "    with device('/cpu:0'):\n",
    "        augmented = rand_augment(transposed)\n",
    "    inputs[\"pixel_values\"] = tf.transpose(augmented, perm=[0,3,2,1])\n",
    "    inputs[\"labels\"] = batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "dataset = load_dataset(\"streetview_images_cropped\", data_dir=\"./\")\n",
    "\n",
    "eval_size=.15\n",
    "test_size=.05\n",
    "\n",
    "dataset = dataset[\"train\"].shuffle().train_test_split(test_size=test_size)\n",
    "dataset_final_test = dataset['test'].with_transform(transform)\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=eval_size)\n",
    "dataset['train'] = dataset['train'].with_transform(augment)\n",
    "dataset['test'] = dataset['test'].with_transform(transform)\n",
    "processed_dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "import tensorflow as tf\n",
    "\n",
    "class_labels = processed_dataset['train'].features[\"label\"].names\n",
    "num_images_train = processed_dataset['train'].num_rows\n",
    "id2label = {str(i): label for i, label in enumerate(class_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_train_epochs = 20\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "learning_rate = 6e-5\n",
    "adam_beta1 = 0.85 # 0.9\n",
    "adam_beta2 = 0.95 # 0.999\n",
    "weight_decay_rate=0.01\n",
    "num_warmup_steps=20\n",
    "output_dir=model_id.split(\"/\")[1]\n",
    "hub_token = HfFolder.get_token()\n",
    "hub_model_id = f'dl-au-tamas-jedrek/{model_id.split(\"/\")[1]}-street-view'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/distances.json\", \"r\") as infile:\n",
    "    distances = json.load(infile)\n",
    "\n",
    "#make matrix with label2id\n",
    "import numpy as np\n",
    "mat_distances = np.zeros((len(label2id), len(label2id)))\n",
    "for key in distances.keys():\n",
    "    for key2 in distances[key].keys():\n",
    "        mat_distances[int(label2id[key])][int(label2id[key2])] = distances[key][key2]\n",
    "mat_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model, specify loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFViTModel\n",
    "\n",
    "base_model = TFViTModel.from_pretrained(model_id, output_attentions = True)\n",
    "pixel_values = tf.keras.layers.Input(shape=(3,224,224), name='pixel_values', dtype='float32')\n",
    "vit = base_model.vit(pixel_values)[0]\n",
    "classifier = tf.keras.layers.Dense(len(class_labels), activation='softmax', name='outputs')(vit[:, 0, :])\n",
    "model = tf.keras.Model(inputs=pixel_values, outputs=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tensor_distances = tf.convert_to_tensor(mat_distances, dtype=tf.float32)\n",
    "def calculate_all_distance(y_true, y_pred):\n",
    "    y_true_indices = tf.range(tf.shape(y_true)[0])\n",
    "    indexed_distances = tf.gather(tensor_distances, y_true_indices)\n",
    "    multiplied = tf.multiply(y_pred, indexed_distances)\n",
    "    dist = tf.reduce_sum(multiplied, axis=1)\n",
    "    return dist\n",
    "def calculate_best_distance(y_true, y_pred):\n",
    "    y_pred_label = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    indices = tf.stack((y_true, y_pred_label), axis=1)\n",
    "    dist = tf.gather_nd(tensor_distances, indices)\n",
    "    return dist\n",
    "\n",
    "def customLoss(y_true, y_pred):\n",
    "    dist = calculate_all_distance(y_true, y_pred)\n",
    "    return (dist ** 2)/500\n",
    "\n",
    "#loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss = customLoss\n",
    "\n",
    "def best_distance(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    dist = calculate_best_distance(y_true, y_pred)\n",
    "    return tf.reduce_mean(dist)\n",
    "\n",
    "def all_distance(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    dist = calculate_all_distance(y_true, y_pred)\n",
    "    return tf.reduce_mean(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "# create optimizer wight weigh decay\n",
    "num_train_steps = num_images_train * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    adam_beta1=adam_beta1,\n",
    "    adam_beta2=adam_beta2,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "\n",
    ")\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer._decayed_lr(tf.float32)\n",
    "    return lr\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "# define metrics \n",
    "metrics=[\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    best_distance,\n",
    "    all_distance,\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name=\"top-3-accuracy\"),\n",
    "    lr_metric,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = True\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_train_dataset = model.prepare_tf_dataset(processed_dataset['train'], batch_size=train_batch_size, shuffle=True)\n",
    "#tf_eval_dataset = model.prepare_tf_dataset(processed_dataset['test'], batch_size=eval_batch_size, shuffle=True)\n",
    "#tf_test_dataset = model.prepare_tf_dataset(dataset_final_test, batch_size=eval_batch_size, shuffle=True)\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = processed_dataset[\"train\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=train_batch_size,\n",
    "   collate_fn=data_collator)\n",
    "tf_eval_dataset = processed_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator)\n",
    "tf_test_dataset = dataset_final_test.to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to display train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#sample_images, sample_labels = next(iter(tf_train_dataset))\n",
    "#plt.figure(figsize=(10, 10))\n",
    "#for i, image in enumerate(sample_images[:9]):\n",
    "#    ax = plt.subplot(3, 3, i + 1)\n",
    "#    transposed = tf.transpose(image)\n",
    "#    plt.imshow(transposed.numpy())\n",
    "#    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push metrics to hub after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard as TensorboardCallback, EarlyStopping, Callback\n",
    "from huggingface_hub import push_to_hub_keras\n",
    "\n",
    "log_dir = os.path.join(output_dir, \"logs\")\n",
    "class CustomPushToHubCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        push_to_hub_keras(model, hub_model_id, log_dir=log_dir)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(TensorboardCallback(log_dir=log_dir))\n",
    "callbacks.append(CustomPushToHubCallback())\n",
    "#callbacks.append(EarlyStopping(monitor=\"val_accuracy\",patience=1))\n",
    "#callbacks.append(PushToHubCallback(\n",
    "#    output_dir,\n",
    "#    hub_model_id=hub_model_id,\n",
    "#    hub_token=hub_token,\n",
    "#))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_info()\n",
    "train_results = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    callbacks=callbacks,\n",
    "    epochs=num_train_epochs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create partial model to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index = 1  # Attention layer index \n",
    "attention_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=layer_index).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_image = tf_test_dataset.take(1)\n",
    "\n",
    "# Now, you can iterate through the one_image dataset to get the individual image and label\n",
    "for image_batch, label_batch in one_image:\n",
    "    # Assuming image_batch has shape (32, 3, 224, 224) and label_batch has shape (32,)\n",
    "    # You can select one image from the batch, for example, the first image:\n",
    "    single_image = image_batch\n",
    "    single_label = label_batch[0]\n",
    "    print(f\"Loaded {single_label}\")\n",
    "\n",
    "\n",
    "preprocessed_image = tf.transpose(single_image[0])\n",
    "# De-normalize the image for visual clarity.\n",
    "in1k_mean = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "in1k_std = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "preprocessed_img_orig = (preprocessed_image * in1k_std) + in1k_mean\n",
    "preprocessed_img_orig = preprocessed_img_orig / 255.0\n",
    "preprocessed_img_orig = tf.clip_by_value(preprocessed_img_orig, 0.0, 1.0)\n",
    "preprocessed_img_orig = tf.image.flip_left_right(preprocessed_img_orig)\n",
    "preprocessed_img_orig = tf.image.rot90(preprocessed_img_orig).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = attention_model(single_image)\n",
    "attention_score = result.attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display attention heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def attention_heatmap(attention_scores, image, model_type=\"dino\"):\n",
    "    num_tokens = 2 if \"distilled\" in model_type else 1\n",
    "    batch_size = 32\n",
    "    num_heads = 12\n",
    "    patch_size = 16\n",
    "\n",
    "    # Process the attention maps for overlay.\n",
    "    w_featmap = 224 // patch_size\n",
    "    h_featmap = 224 // patch_size\n",
    "\n",
    "    # Taking the representations from CLS token.\n",
    "    attentions = attention_scores[0, :, 1, num_tokens:].numpy().reshape(num_heads, -1)\n",
    "\n",
    "    # Reshape the attention scores to resemble mini patches.\n",
    "    attentions = attentions.reshape(num_heads, w_featmap, h_featmap)\n",
    "    attentions = attentions.transpose((1, 2, 0))\n",
    "\n",
    "    # Resize the attention patches to 224x224 (224: 14x16).\n",
    "    attentions = tf.image.resize(\n",
    "        attentions, size=(224,224)\n",
    "    ) \n",
    "    return attentions\n",
    "\n",
    "# Generate the attention heatmaps.\n",
    "attentions = attention_heatmap(attention_score[0], preprocessed_img_orig)\n",
    "\n",
    "# Plot the maps.\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(13, 13))\n",
    "img_count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if img_count < len(attentions):\n",
    "            axes[i, j].imshow(preprocessed_img_orig)\n",
    "            axes[i, j].imshow(attentions[..., img_count], cmap=\"inferno\", alpha=0.5)\n",
    "            axes[i, j].title.set_text(f\"Attention head: {img_count}\")\n",
    "            axes[i, j].axis(\"off\")\n",
    "            img_count += 1\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(3, 3, 1)\n",
    "plt.imshow(preprocessed_img_orig)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "predictions = np.array([])\n",
    "true_labels = np.array([])\n",
    "\n",
    "for x, y in tf_test_dataset:\n",
    "    y_prob = model.predict(x)\n",
    "    # Apply softmax to obtain probabilities\n",
    "    probabilities = tf.nn.softmax(y_prob.logits, axis=-1).numpy()\n",
    "    # Get the predicted labels (class with the highest probability)\n",
    "    y_pred = tf.argmax(probabilities, axis=-1).numpy()\n",
    "\n",
    "    predictions = np.concatenate([predictions, y_pred])\n",
    "    print(y_pred)\n",
    "    print(y.numpy())\n",
    "    true_labels = np.concatenate([true_labels, y.numpy()])\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['font.size'] = 1\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
