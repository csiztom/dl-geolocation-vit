{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip install keras_cv datasets transformers tensorboard tensorflow ipywidgets opencv-python tensorflow-datasets scikit-learn\n",
        "!git-lfs --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Login to huggingface if first time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show that the GPU is being used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.config.experimental import list_physical_devices\n",
        "print(list_physical_devices('GPU'))\n",
        "\n",
        "model_id = \"google/vit-base-patch16-224-in21k\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the database, also this is the time to define data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa78f4d7f8164186b9905be3b8371d67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/42570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import ViTImageProcessor\n",
        "from datasets import load_dataset\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras_cv.layers import RandAugment\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the model ID and other parameters\n",
        "num_layers = 2\n",
        "magnitude = 0.15\n",
        "\n",
        "# Load the ViTImageProcessor\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Create RandAugment transformation\n",
        "rand_augment = RandAugment(\n",
        "    value_range=[-1,1],\n",
        "    augmentations_per_image=num_layers,\n",
        "    magnitude=magnitude,\n",
        ")\n",
        "\n",
        "\n",
        "def transform(batch):\n",
        "    inputs = image_processor([x for x in batch[\"image\"]], return_tensors=\"tf\")\n",
        "    inputs[\"labels\"] = batch[\"label\"]\n",
        "    return inputs\n",
        "\n",
        "def augment(batch):\n",
        "    inputs = image_processor([x for x in batch[\"image\"]], return_tensors=\"tf\")\n",
        "    transposed = tf.transpose(inputs[\"pixel_values\"], perm=[0,3,2,1])\n",
        "    augmented = rand_augment(transposed)\n",
        "    inputs[\"pixel_values\"] = tf.transpose(augmented, perm=[0,3,2,1])\n",
        "    inputs[\"labels\"] = batch[\"label\"]\n",
        "    return inputs\n",
        "\n",
        "dataset = load_dataset(\"streetview_images_cropped\", data_dir=\"./\")\n",
        "\n",
        "eval_size=.15\n",
        "test_size=.05\n",
        "\n",
        "dataset = dataset[\"train\"].shuffle().train_test_split(test_size=test_size)\n",
        "dataset_final_test = dataset['test'].with_transform(transform)\n",
        "\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=eval_size)\n",
        "dataset['train'] = dataset['train'].with_transform(augment)\n",
        "dataset['test'] = dataset['test'].with_transform(transform)\n",
        "processed_dataset = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder\n",
        "import tensorflow as tf\n",
        "\n",
        "class_labels = processed_dataset['train'].features[\"label\"].names\n",
        "num_images_train = processed_dataset['train'].num_rows\n",
        "id2label = {str(i): label for i, label in enumerate(class_labels)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "num_train_epochs = 5\n",
        "train_batch_size = 32\n",
        "eval_batch_size = 32\n",
        "learning_rate = 6e-5\n",
        "weight_decay_rate=0.01\n",
        "num_warmup_steps=0\n",
        "output_dir=model_id.split(\"/\")[1]\n",
        "hub_token = HfFolder.get_token()\n",
        "hub_model_id = f'dl-au-tamas-jedrek/{model_id.split(\"/\")[1]}-street-view'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 2.12150204, 2.10150743, ..., 1.43083091, 2.64981238,\n",
              "        1.05595098],\n",
              "       [2.12150204, 0.        , 0.0631969 , ..., 0.7217327 , 2.86360224,\n",
              "        2.20667979],\n",
              "       [2.10150743, 0.0631969 , 0.        , ..., 0.68933549, 2.90697122,\n",
              "        2.21733372],\n",
              "       ...,\n",
              "       [1.43083091, 0.7217327 , 0.68933549, ..., 0.        , 2.77736714,\n",
              "        1.73784865],\n",
              "       [2.64981238, 2.86360224, 2.90697122, ..., 2.77736714, 0.        ,\n",
              "        1.60919535],\n",
              "       [1.05595098, 2.20667979, 2.21733372, ..., 1.73784865, 1.60919535,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"data/distances.json\", \"r\") as infile:\n",
        "    distances = json.load(infile)\n",
        "\n",
        "#make matrix with label2id\n",
        "import numpy as np\n",
        "mat_distances = np.zeros((len(label2id), len(label2id)))\n",
        "for key in distances.keys():\n",
        "    for key2 in distances[key].keys():\n",
        "        mat_distances[int(label2id[key])][int(label2id[key2])] = distances[key][key2]\n",
        "mat_distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get model, specify loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-19 12:01:18.730850: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
            "2023-11-19 12:01:18.730870: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2023-11-19 12:01:18.730873: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2023-11-19 12:01:18.730901: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-11-19 12:01:18.730917: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n",
            "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_vi_t_for_image_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vit (TFViTMainLayer)        multiple                  85798656  \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  99201     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85897857 (327.67 MB)\n",
            "Trainable params: 85897857 (327.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFViTForImageClassification, create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# create optimizer wight weigh decay\n",
        "num_train_steps = num_images_train * num_train_epochs\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=weight_decay_rate,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        ")\n",
        "\n",
        "# load pre-trained ViT model\n",
        "model = TFViTForImageClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=len(class_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "tensor_distances = tf.convert_to_tensor(mat_distances, dtype=tf.float32)\n",
        "def customLoss(y_true, y_pred):\n",
        "    y_pred_label = tf.argmax(y_pred, axis=1)\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    indices = tf.stack((y_true, y_pred_label), axis=1)\n",
        "    dist = tf.gather_nd(tensor_distances, indices)\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) + (2 * dist)\n",
        "# define loss\n",
        "#loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss = customLoss\n",
        "\n",
        "# define metrics \n",
        "metrics=[\n",
        "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "    tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name=\"top-3-accuracy\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_vi_t_for_image_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vit (TFViTMainLayer)        multiple                  85798656  \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  99201     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85897857 (327.67 MB)\n",
            "Trainable params: 85897857 (327.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.vit.embeddings.trainable = True\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'rand_augment' (type RandAugment).\n\nin user code:\n\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/rand_augment.py\", line 127, in _augment  *\n        result = super()._augment(sample)\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/random_augmentation_pipeline.py\", line 104, in _augment  *\n        result = tf.cond(\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py\", line 420, in call\n        outputs = self._format_output(self._augment(inputs), metadata)\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/random_choice.py\", line 90, in _augment\n        selected_op = self._random_generator.random_uniform(\n\n    InvalidArgumentError: Exception encountered when calling layer 'random_choice' (type RandomChoice).\n    \n    {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0-D, got shape [2]\n    \t [[{{node StatelessRandomUniformIntV2}}]] [Op:StatelessRandomUniformIntV2] name: \n    \n    Call arguments received by layer 'random_choice' (type RandomChoice):\n      • inputs={'images': 'tf.Tensor(shape=(224, 224, 3), dtype=float32)'}\n\n\nCall arguments received by layer 'rand_augment' (type RandAugment):\n  • inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf_train_dataset \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mprepare_tf_dataset(processed_dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], batch_size\u001b[39m=\u001b[39;49mtrain_batch_size, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tf_eval_dataset \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mprepare_tf_dataset(processed_dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m], batch_size\u001b[39m=\u001b[39meval_batch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tf_test_dataset \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mprepare_tf_dataset(dataset_final_test, batch_size\u001b[39m=\u001b[39meval_batch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1454\u001b[0m, in \u001b[0;36mTFPreTrainedModel.prepare_tf_dataset\u001b[0;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[1;32m   1452\u001b[0m model_labels \u001b[39m=\u001b[39m find_labels(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m   1453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcols_to_retain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(inspect\u001b[39m.\u001b[39msignature(dataset\u001b[39m.\u001b[39m_get_output_signature)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m-> 1454\u001b[0m     output_signature, _ \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m_get_output_signature(\n\u001b[1;32m   1455\u001b[0m         dataset,\n\u001b[1;32m   1456\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1457\u001b[0m         collate_fn\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[1;32m   1458\u001b[0m         collate_fn_args\u001b[39m=\u001b[39;49mcollate_fn_args,\n\u001b[1;32m   1459\u001b[0m         cols_to_retain\u001b[39m=\u001b[39;49mmodel_inputs,\n\u001b[1;32m   1460\u001b[0m     )\n\u001b[1;32m   1461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1462\u001b[0m     \u001b[39m# TODO Matt: This is a workaround for older versions of datasets that are missing the `cols_to_retain`\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m     \u001b[39m#            argument. We should remove this once the minimum supported version of datasets is > 2.3.2\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m     unwanted_columns \u001b[39m=\u001b[39m [\n\u001b[1;32m   1465\u001b[0m         feature\n\u001b[1;32m   1466\u001b[0m         \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mfeatures\n\u001b[1;32m   1467\u001b[0m         \u001b[39mif\u001b[39;00m feature \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_inputs \u001b[39mand\u001b[39;00m feature \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1468\u001b[0m     ]\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/datasets/arrow_dataset.py:275\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin._get_output_signature\u001b[0;34m(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_test_batches):\n\u001b[1;32m    274\u001b[0m     indices \u001b[39m=\u001b[39m sample(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)), test_batch_size)\n\u001b[0;32m--> 275\u001b[0m     test_batch \u001b[39m=\u001b[39m dataset[indices]\n\u001b[1;32m    276\u001b[0m     \u001b[39mif\u001b[39;00m cols_to_retain \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m         test_batch \u001b[39m=\u001b[39m {key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m test_batch\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m cols_to_retain}\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/datasets/arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/datasets/arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2787\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2788\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2790\u001b[0m )\n\u001b[1;32m   2791\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    630\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/datasets/formatting/formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_batch(pa_table)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/datasets/formatting/formatting.py:515\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    513\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    514\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 515\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(batch)\n",
            "\u001b[1;32m/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m inputs \u001b[39m=\u001b[39m image_processor([x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m transposed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(inputs[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m], perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m augmented \u001b[39m=\u001b[39m rand_augment(transposed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m inputs[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(augmented, perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/au724747/projects/dl-geolocation-vit/image-classification.ipynb#X51sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m inputs[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:423\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_augment(inputs), metadata)\n\u001b[1;32m    421\u001b[0m \u001b[39melif\u001b[39;00m images\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    422\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_output(\n\u001b[0;32m--> 423\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_augment(inputs), metadata\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImage augmentation layers are expecting inputs to be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrank 3 (HWC) or 4D (NHWC) tensors. Got shape: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mimages\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:515\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._batch_augment\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_augment\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_augment, inputs)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:288\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._map_fn\u001b[0;34m(self, func, inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_vectorize:\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mvectorized_map(func, inputs)\n\u001b[0;32m--> 288\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mmap_fn(func, inputs)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'rand_augment' (type RandAugment).\n\nin user code:\n\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/rand_augment.py\", line 127, in _augment  *\n        result = super()._augment(sample)\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/random_augmentation_pipeline.py\", line 104, in _augment  *\n        result = tf.cond(\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py\", line 420, in call\n        outputs = self._format_output(self._augment(inputs), metadata)\n    File \"/opt/homebrew/anaconda3/envs/tf/lib/python3.9/site-packages/keras_cv/src/layers/preprocessing/random_choice.py\", line 90, in _augment\n        selected_op = self._random_generator.random_uniform(\n\n    InvalidArgumentError: Exception encountered when calling layer 'random_choice' (type RandomChoice).\n    \n    {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0-D, got shape [2]\n    \t [[{{node StatelessRandomUniformIntV2}}]] [Op:StatelessRandomUniformIntV2] name: \n    \n    Call arguments received by layer 'random_choice' (type RandomChoice):\n      • inputs={'images': 'tf.Tensor(shape=(224, 224, 3), dtype=float32)'}\n\n\nCall arguments received by layer 'rand_augment' (type RandAugment):\n  • inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)"
          ]
        }
      ],
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(processed_dataset['train'], batch_size=train_batch_size, shuffle=True)\n",
        "tf_eval_dataset = model.prepare_tf_dataset(processed_dataset['test'], batch_size=eval_batch_size, shuffle=True)\n",
        "tf_test_dataset = model.prepare_tf_dataset(dataset_final_test, batch_size=eval_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run to display train images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_images, sample_labels = next(iter(tf_train_dataset))\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, image in enumerate(sample_images[:9]):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    transposed = tf.transpose(image)\n",
        "    plt.imshow(transposed.numpy())\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Push metrics to hub after every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from tensorflow.keras.callbacks import TensorBoard as TensorboardCallback, EarlyStopping\n",
        "\n",
        "callbacks = []\n",
        "callbacks.append(TensorboardCallback(log_dir=os.path.join(output_dir, \"logs\")))\n",
        "#callbacks.append(EarlyStopping(monitor=\"val_accuracy\",patience=1))\n",
        "callbacks.append(PushToHubCallback(\n",
        "    output_dir,\n",
        "    hub_model_id=hub_model_id,\n",
        "    hub_token=hub_token,\n",
        "))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import logging as transformers_logging\n",
        "transformers_logging.set_verbosity_info()\n",
        "train_results = model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_eval_dataset,\n",
        "    callbacks=callbacks,\n",
        "    epochs=num_train_epochs,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "predictions = np.array([])\n",
        "true_labels = np.array([])\n",
        "\n",
        "for x, y in tf_test_dataset:\n",
        "    y_prob = model.predict(x)\n",
        "    # Apply softmax to obtain probabilities\n",
        "    probabilities = tf.nn.softmax(y_prob.logits, axis=-1).numpy()\n",
        "    # Get the predicted labels (class with the highest probability)\n",
        "    y_pred = tf.argmax(probabilities, axis=-1).numpy()\n",
        "\n",
        "    predictions = np.concatenate([predictions, y_pred])\n",
        "    print(y_pred)\n",
        "    print(y.numpy())\n",
        "    true_labels = np.concatenate([true_labels, y.numpy()])\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "interpreter": {
      "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
