{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip install keras_cv datasets transformers tensorboard tensorflow ipywidgets opencv-python tensorflow-datasets scikit-learn\n",
        "!git-lfs --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Login to huggingface if first time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show that the GPU is being used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.config.experimental import list_physical_devices\n",
        "print(list_physical_devices('GPU'))\n",
        "\n",
        "model_id = \"google/vit-base-patch16-224-in21k\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the database, also this is the time to define data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "519c78c3f7bf4e28ad616deda5f3236e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/42570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import ViTImageProcessor\n",
        "from datasets import load_dataset\n",
        "from tensorflow import device\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras_cv.layers import RandAugment\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the model ID and other parameters\n",
        "num_layers = 2\n",
        "magnitude = 0.15\n",
        "\n",
        "# Load the ViTImageProcessor\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Create RandAugment transformation\n",
        "rand_augment = RandAugment(\n",
        "    value_range=[-1,1],\n",
        "    augmentations_per_image=num_layers,\n",
        "    magnitude=magnitude,\n",
        ")\n",
        "\n",
        "\n",
        "def transform(batch):\n",
        "    inputs = image_processor([x for x in batch[\"image\"]], return_tensors=\"tf\")\n",
        "    inputs[\"labels\"] = batch[\"label\"]\n",
        "    return inputs\n",
        "\n",
        "def augment(batch):\n",
        "    inputs = image_processor([x for x in batch[\"image\"]], return_tensors=\"tf\")\n",
        "    transposed = tf.transpose(inputs[\"pixel_values\"], perm=[0,3,2,1])\n",
        "    with device('/cpu:0'):\n",
        "        augmented = rand_augment(transposed)\n",
        "    inputs[\"pixel_values\"] = tf.transpose(augmented, perm=[0,3,2,1])\n",
        "    inputs[\"labels\"] = batch[\"label\"]\n",
        "    return inputs\n",
        "\n",
        "dataset = load_dataset(\"streetview_images_cropped\", data_dir=\"./\")\n",
        "\n",
        "eval_size=.15\n",
        "test_size=.05\n",
        "\n",
        "dataset = dataset[\"train\"].shuffle().train_test_split(test_size=test_size)\n",
        "dataset_final_test = dataset['test'].with_transform(transform)\n",
        "\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=eval_size)\n",
        "dataset['train'] = dataset['train'].with_transform(augment)\n",
        "dataset['test'] = dataset['test'].with_transform(transform)\n",
        "processed_dataset = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder\n",
        "import tensorflow as tf\n",
        "\n",
        "class_labels = processed_dataset['train'].features[\"label\"].names\n",
        "num_images_train = processed_dataset['train'].num_rows\n",
        "id2label = {str(i): label for i, label in enumerate(class_labels)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "num_train_epochs = 20\n",
        "train_batch_size = 32\n",
        "eval_batch_size = 32\n",
        "learning_rate = 6e-5\n",
        "adam_beta1 = 0.85 # 0.9\n",
        "adam_beta2 = 0.95 # 0.999\n",
        "weight_decay_rate=0.01\n",
        "num_warmup_steps=20\n",
        "output_dir=model_id.split(\"/\")[1]\n",
        "hub_token = HfFolder.get_token()\n",
        "hub_model_id = f'dl-au-tamas-jedrek/{model_id.split(\"/\")[1]}-street-view'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0.        , 229.02150892, 228.10446099, ..., 157.48504833,\n",
              "        168.92791079,  68.51475158],\n",
              "       [229.02150892,   0.        ,   3.83173362, ...,  72.6392153 ,\n",
              "        257.10553454, 242.08400282],\n",
              "       [228.10446099,   3.83173362,   0.        , ...,  71.2555174 ,\n",
              "        258.7988651 , 242.2555515 ],\n",
              "       ...,\n",
              "       [157.48504833,  72.6392153 ,  71.2555174 , ...,   0.        ,\n",
              "        215.85521045, 178.68501914],\n",
              "       [168.92791079, 257.10553454, 258.7988651 , ..., 215.85521045,\n",
              "          0.        , 103.66397521],\n",
              "       [ 68.51475158, 242.08400282, 242.2555515 , ..., 178.68501914,\n",
              "        103.66397521,   0.        ]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"data/distances.json\", \"r\") as infile:\n",
        "    distances = json.load(infile)\n",
        "\n",
        "#make matrix with label2id\n",
        "import numpy as np\n",
        "mat_distances = np.zeros((len(label2id), len(label2id)))\n",
        "for key in distances.keys():\n",
        "    for key2 in distances[key].keys():\n",
        "        mat_distances[int(label2id[key])][int(label2id[key2])] = distances[key][key2]\n",
        "normalize\n",
        "factor = np.max(mat_distances)\n",
        "mat_distances = mat_distances / factor\n",
        "mat_distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get model, specify loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-30 10:09:30.901655: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
            "2023-11-30 10:09:30.901732: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2023-11-30 10:09:30.901743: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2023-11-30 10:09:30.902185: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-11-30 10:09:30.902599: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "All model checkpoint layers were used when initializing TFViTModel.\n",
            "\n",
            "All the layers of TFViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFViTModel\n",
        "\n",
        "base_model = TFViTModel.from_pretrained(model_id, output_attentions = True)\n",
        "pixel_values = tf.keras.layers.Input(shape=(3,224,224), name='pixel_values', dtype='float32')\n",
        "vit = base_model.vit(pixel_values)[0]\n",
        "classifier = tf.keras.layers.Dense(len(class_labels), activation='softmax', name='outputs')(vit[:, 0, :])\n",
        "model = tf.keras.Model(inputs=pixel_values, outputs=classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tensor_distances = tf.convert_to_tensor(mat_distances, dtype=tf.float32)\n",
        "def calculate_all_distance(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    indexed_distances = tf.gather(tensor_distances, y_true)\n",
        "    multiplied = tf.multiply(y_pred, indexed_distances)\n",
        "    dist = tf.reduce_sum(multiplied, axis=1)\n",
        "    return dist\n",
        "def calculate_best_distance(y_true, y_pred):\n",
        "    y_pred_label = tf.argmax(y_pred, axis=1)\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    indices = tf.stack((y_true, y_pred_label), axis=1)\n",
        "    dist = tf.gather_nd(tensor_distances, indices)\n",
        "    return dist\n",
        "\n",
        "def customLoss(y_true, y_pred):\n",
        "    dist = calculate_all_distance(y_true, y_pred)\n",
        "    return (dist ** 2)/500\n",
        "\n",
        "#loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "loss = customLoss\n",
        "\n",
        "def best_distance(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    dist = calculate_best_distance(y_true, y_pred)\n",
        "    return tf.reduce_mean(dist) * factor\n",
        "\n",
        "def all_distance(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    dist = calculate_all_distance(y_true, y_pred)\n",
        "    return tf.reduce_mean(dist) * factor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "# create optimizer wight weigh decay\n",
        "num_train_steps = num_images_train * num_train_epochs\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    adam_beta1=adam_beta1,\n",
        "    adam_beta2=adam_beta2,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=weight_decay_rate,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "\n",
        ")\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer._decayed_lr(tf.float32)\n",
        "    return lr\n",
        "lr_metric = get_lr_metric(optimizer)\n",
        "\n",
        "# define metrics \n",
        "metrics=[\n",
        "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "    best_distance,\n",
        "    all_distance,\n",
        "    tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name=\"top-3-accuracy\"),\n",
        "    lr_metric,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.layers[1].trainable = True\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tf_train_dataset = model.prepare_tf_dataset(processed_dataset['train'], batch_size=train_batch_size, shuffle=True)\n",
        "#tf_eval_dataset = model.prepare_tf_dataset(processed_dataset['test'], batch_size=eval_batch_size, shuffle=True)\n",
        "#tf_test_dataset = model.prepare_tf_dataset(dataset_final_test, batch_size=eval_batch_size, shuffle=True)\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "tf_train_dataset = processed_dataset[\"train\"].to_tf_dataset(\n",
        "   columns=['pixel_values'],\n",
        "   label_cols=[\"labels\"],\n",
        "   shuffle=True,\n",
        "   batch_size=train_batch_size,\n",
        "   collate_fn=data_collator)\n",
        "tf_eval_dataset = processed_dataset[\"test\"].to_tf_dataset(\n",
        "    columns=['pixel_values'],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=eval_batch_size,\n",
        "    collate_fn=data_collator)\n",
        "tf_test_dataset = dataset_final_test.to_tf_dataset(\n",
        "    columns=['pixel_values'],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=eval_batch_size,\n",
        "    collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run to display train images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#sample_images, sample_labels = next(iter(tf_train_dataset))\n",
        "#plt.figure(figsize=(10, 10))\n",
        "#for i, image in enumerate(sample_images[:9]):\n",
        "#    ax = plt.subplot(3, 3, i + 1)\n",
        "#    transposed = tf.transpose(image)\n",
        "#    plt.imshow(transposed.numpy())\n",
        "#    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Push metrics to hub after every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from tensorflow.keras.callbacks import TensorBoard as TensorboardCallback, EarlyStopping, Callback\n",
        "from huggingface_hub import push_to_hub_keras\n",
        "\n",
        "log_dir = os.path.join(output_dir, \"logs\")\n",
        "class CustomPushToHubCallback(Callback):\n",
        "    def on_train_end(self, epoch, logs=None):\n",
        "        push_to_hub_keras(model, hub_model_id, log_dir=log_dir)\n",
        "\n",
        "callbacks = []\n",
        "callbacks.append(TensorboardCallback(log_dir=log_dir))\n",
        "callbacks.append(CustomPushToHubCallback())\n",
        "#callbacks.append(EarlyStopping(monitor=\"val_accuracy\",patience=1))\n",
        "#callbacks.append(PushToHubCallback(\n",
        "#    output_dir,\n",
        "#    hub_model_id=hub_model_id,\n",
        "#    hub_token=hub_token,\n",
        "#))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import logging as transformers_logging\n",
        "transformers_logging.set_verbosity_info()\n",
        "train_results = model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_eval_dataset,\n",
        "    callbacks=callbacks,\n",
        "    epochs=num_train_epochs,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create partial model to display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer_index = 1  # Attention layer index \n",
        "attention_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=layer_index).output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "one_image = tf_test_dataset.take(1)\n",
        "\n",
        "# Now, you can iterate through the one_image dataset to get the individual image and label\n",
        "for image_batch, label_batch in one_image:\n",
        "    # Assuming image_batch has shape (32, 3, 224, 224) and label_batch has shape (32,)\n",
        "    # You can select one image from the batch, for example, the first image:\n",
        "    single_image = image_batch\n",
        "    single_label = label_batch[0]\n",
        "    print(f\"Loaded {single_label}\")\n",
        "\n",
        "\n",
        "preprocessed_image = tf.transpose(single_image[0])\n",
        "# De-normalize the image for visual clarity.\n",
        "in1k_mean = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
        "in1k_std = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
        "preprocessed_img_orig = (preprocessed_image * in1k_std) + in1k_mean\n",
        "preprocessed_img_orig = preprocessed_img_orig / 255.0\n",
        "preprocessed_img_orig = tf.clip_by_value(preprocessed_img_orig, 0.0, 1.0)\n",
        "preprocessed_img_orig = tf.image.flip_left_right(preprocessed_img_orig)\n",
        "preprocessed_img_orig = tf.image.rot90(preprocessed_img_orig).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get attention scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = attention_model(single_image)\n",
        "attention_score = result.attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display attention heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def attention_heatmap(attention_scores, image, model_type=\"dino\"):\n",
        "    num_tokens = 2 if \"distilled\" in model_type else 1\n",
        "    batch_size = 32\n",
        "    num_heads = 12\n",
        "    patch_size = 16\n",
        "\n",
        "    # Process the attention maps for overlay.\n",
        "    w_featmap = 224 // patch_size\n",
        "    h_featmap = 224 // patch_size\n",
        "\n",
        "    # Taking the representations from CLS token.\n",
        "    attentions = attention_scores[0, :, 1, num_tokens:].numpy().reshape(num_heads, -1)\n",
        "\n",
        "    # Reshape the attention scores to resemble mini patches.\n",
        "    attentions = attentions.reshape(num_heads, w_featmap, h_featmap)\n",
        "    attentions = attentions.transpose((1, 2, 0))\n",
        "\n",
        "    # Resize the attention patches to 224x224 (224: 14x16).\n",
        "    attentions = tf.image.resize(\n",
        "        attentions, size=(224,224)\n",
        "    ) \n",
        "    return attentions\n",
        "\n",
        "# Generate the attention heatmaps.\n",
        "attentions = attention_heatmap(attention_score[0], preprocessed_img_orig)\n",
        "\n",
        "# Plot the maps.\n",
        "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(13, 13))\n",
        "img_count = 0\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(4):\n",
        "        if img_count < len(attentions):\n",
        "            axes[i, j].imshow(preprocessed_img_orig)\n",
        "            axes[i, j].imshow(attentions[..., img_count], cmap=\"inferno\", alpha=0.5)\n",
        "            axes[i, j].title.set_text(f\"Attention head: {img_count}\")\n",
        "            axes[i, j].axis(\"off\")\n",
        "            img_count += 1\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = plt.subplot(3, 3, 1)\n",
        "plt.imshow(preprocessed_img_orig)\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "predictions = np.array([])\n",
        "true_labels = np.array([])\n",
        "\n",
        "for x, y in tf_test_dataset:\n",
        "    y_prob = model.predict(x)\n",
        "    # Apply softmax to obtain probabilities\n",
        "    probabilities = tf.nn.softmax(y_prob.logits, axis=-1).numpy()\n",
        "    # Get the predicted labels (class with the highest probability)\n",
        "    y_pred = tf.argmax(probabilities, axis=-1).numpy()\n",
        "\n",
        "    predictions = np.concatenate([predictions, y_pred])\n",
        "    print(y_pred)\n",
        "    print(y.numpy())\n",
        "    true_labels = np.concatenate([true_labels, y.numpy()])\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "plt.rcParams['figure.dpi'] = 600\n",
        "plt.rcParams['font.size'] = 1\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
